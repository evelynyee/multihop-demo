{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install inspect-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from inspect_ai import Task, task\n",
    "from inspect_ai import eval as evaluate_model\n",
    "from inspect_ai.dataset import json_dataset, Sample\n",
    "from inspect_ai.scorer import includes, answer, Score, accuracy, stderr, scorer\n",
    "from inspect_ai.solver import generate, prompt_template\n",
    "from inspect_ai.log import list_eval_logs, read_eval_log\n",
    "\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import statsmodels.api as sm\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "## Source: [MoreHopQA final](https://github.com/Alab-NII/morehopqa/blob/main/datasets/files/morehopqa_final.json), letter-only\n",
    "I loaded the data from the github repo, filtered to questions which had a “letter” answer type, in order to have consistent answer parsing. This resulted in 43 multi-hop questions.\n",
    "I also parsed the provided “context” for each question into the following format:\n",
    "- {topic title}\n",
    "\t- {topic sentence}\n",
    "\t- {topic sentence}\n",
    "\n",
    "Note: I was originally hoping to use the MoreHopQA dataset without the context, but the model seemed unable to get answer any of the questions correctly due to factual recall errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 43 samples.\n",
      "Example sample:  {\n",
      "  \"previous_question\": \"Who was the producer of the 1994 American animated epic musical film which Mel Shaw animated?\",\n",
      "  \"previous_answer\": \"Don Hahn\",\n",
      "  \"no_of_hops\": 2,\n",
      "  \"answer_type\": \"letter\",\n",
      "  \"context\": \"Mel Shaw:\\n\\t- Mel Shaw (December 19, 1914 \\u2013 November 22, 2012) was an American animator, design artist, writer, and artist.\\n\\t- Shaw was involved in the animation, story design, and visual development of numerous Disney animated films, beginning with \\\"Bambi\\\", which was released in 1942.\\n\\t- His other animated film credits, usually involving animation design or the story, included \\\"The Rescuers\\\" in 1977, \\\"The Fox and the Hound\\\" in 1981, \\\"The Black Cauldron\\\" in 1985, \\\"The Great Mouse Detective\\\" in 1986, \\\"Beauty and the Beast\\\" in 1991, and \\\"The Lion King\\\" in 1994.\\n\\t- He was named a Disney Legend in 2004 for his contributions to the Walt Disney Company.\\n- The Lion King:\\n\\t- The Lion King is a 1994 American animated epic musical film produced by Walt Disney Feature Animation and released by Walt Disney Pictures.\\n\\t- It is the 32nd Disney animated feature film, and the fifth animated film produced during a period known as the Disney Renaissance.\\n\\t- \\\"The Lion King\\\" was directed by Roger Allers and Rob Minkoff, produced by Don Hahn, and has a screenplay credited to Irene Mecchi, Jonathan Roberts, and Linda Woolverton.\\n\\t- Its original songs were written by composer Elton John and lyricist Tim Rice, and original scores were written by Hans Zimmer.\\n\\t- The film features an ensemble voice cast that includes Matthew Broderick, James Earl Jones, Jeremy Irons, Jonathan Taylor Thomas, Moira Kelly, Nathan Lane, Ernie Sabella, Rowan Atkinson, Robert Guillaume, Madge Sinclair, Whoopi Goldberg, Cheech Marin, and Jim Cummings.\\n\\t- The story takes place in a kingdom of lions in Africa and was influenced by William Shakespeare's \\\"Hamlet\\\".\",\n",
      "  \"question\": \"What is the last letter of the last name of the producer of the 1994 American animated epic musical film which Mel Shaw animated?\",\n",
      "  \"answer\": \"n\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "demo_path = \"demo_data_letter.json\"\n",
    "og_data_path = os.path.abspath(os.path.join(\"..\", \"morehopqa\",\"datasets\",\"files\",\"morehopqa_final.json\"))\n",
    "if not os.path.exists(demo_path): # select 5 samples for demo\n",
    "    with open(og_data_path,'r', encoding='utf-8') as f:\n",
    "        d = json.load(f)\n",
    "    d_filt = [s for s in d if s['answer_type'] == 'letter'] # letter, to give it a better chance?\n",
    "    demo_data = d_filt\n",
    "    # demo_data = np.random.choice(d_filt,5,replace=False).tolist()\n",
    "    print(demo_data)\n",
    "    with open(demo_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(demo_data,f, indent=2)\n",
    "\n",
    "# load into inspect framework\n",
    "def process_morehopqa (record):\n",
    "    q = record['question']\n",
    "    context = '\\n- '.join(['\\n\\t- '.join([title+':']+[chunk.strip() for chunk in chunks])\n",
    "                           for title, chunks in record['context']])\n",
    "    a = record['answer']\n",
    "    meta = {k:record[k] for k in ['previous_question',\n",
    "                                                   'previous_answer',\n",
    "                                                   \"no_of_hops\",\n",
    "                                                   \"answer_type\"]}\n",
    "    meta['context'] = context\n",
    "    meta['question'] = q # for prompt template\n",
    "    meta['answer'] = a\n",
    "    return Sample(id=record[\"_id\"],\n",
    "                  input=q,\n",
    "                  target=a,\n",
    "                  metadata=meta\n",
    "                  )\n",
    "data = json_dataset(demo_path,\n",
    "                    sample_fields = process_morehopqa)\n",
    "\n",
    "# # display data\n",
    "print(f\"Dataset has {len(data)} samples.\")\n",
    "print(\"Example sample: \", json.dumps(data.__dict__['samples'][0].__dict__['metadata'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 43 samples.\n",
      "Example sample:  {\n",
      "  \"previous_question\": \"Who was the producer of the 1994 American animated epic musical film which Mel Shaw animated?\",\n",
      "  \"previous_answer\": \"Don Hahn\",\n",
      "  \"no_of_hops\": 1,\n",
      "  \"answer_type\": \"letter\",\n",
      "  \"context\": \"Mel Shaw:\\n\\t- Mel Shaw (December 19, 1914 \\u2013 November 22, 2012) was an American animator, design artist, writer, and artist.\\n\\t- Shaw was involved in the animation, story design, and visual development of numerous Disney animated films, beginning with \\\"Bambi\\\", which was released in 1942.\\n\\t- His other animated film credits, usually involving animation design or the story, included \\\"The Rescuers\\\" in 1977, \\\"The Fox and the Hound\\\" in 1981, \\\"The Black Cauldron\\\" in 1985, \\\"The Great Mouse Detective\\\" in 1986, \\\"Beauty and the Beast\\\" in 1991, and \\\"The Lion King\\\" in 1994.\\n\\t- He was named a Disney Legend in 2004 for his contributions to the Walt Disney Company.\\n- The Lion King:\\n\\t- The Lion King is a 1994 American animated epic musical film produced by Walt Disney Feature Animation and released by Walt Disney Pictures.\\n\\t- It is the 32nd Disney animated feature film, and the fifth animated film produced during a period known as the Disney Renaissance.\\n\\t- \\\"The Lion King\\\" was directed by Roger Allers and Rob Minkoff, produced by Don Hahn, and has a screenplay credited to Irene Mecchi, Jonathan Roberts, and Linda Woolverton.\\n\\t- Its original songs were written by composer Elton John and lyricist Tim Rice, and original scores were written by Hans Zimmer.\\n\\t- The film features an ensemble voice cast that includes Matthew Broderick, James Earl Jones, Jeremy Irons, Jonathan Taylor Thomas, Moira Kelly, Nathan Lane, Ernie Sabella, Rowan Atkinson, Robert Guillaume, Madge Sinclair, Whoopi Goldberg, Cheech Marin, and Jim Cummings.\\n\\t- The story takes place in a kingdom of lions in Africa and was influenced by William Shakespeare's \\\"Hamlet\\\".\",\n",
      "  \"question\": \"Who was the producer of the 1994 American animated epic musical film which Mel Shaw animated?\",\n",
      "  \"answer\": \"Don Hahn\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# get previous question (1 hop)\n",
    "def get_prev(record):\n",
    "    q = record['previous_question']\n",
    "    a = record['previous_answer']\n",
    "    meta = {k:record[k] for k in ['previous_question',\n",
    "                                                   'previous_answer',\n",
    "                                                   \"no_of_hops\",\n",
    "                                                   \"answer_type\"]}\n",
    "    meta['no_of_hops'] = meta['no_of_hops'] - 1\n",
    "    context = '\\n- '.join(['\\n\\t- '.join([title+':']+[chunk.strip() for chunk in chunks])\n",
    "                           for title, chunks in record['context']])\n",
    "    meta['context'] = context\n",
    "    meta['question'] = q # for prompt template\n",
    "    meta['answer'] = a\n",
    "    return Sample(id=record[\"_id\"],\n",
    "                  input=q,\n",
    "                  target=a,\n",
    "                  metadata=meta\n",
    "                  )\n",
    "data_prev = json_dataset(demo_path,\n",
    "                         get_prev)\n",
    "print(f\"Dataset has {len(data_prev)} samples.\")\n",
    "print(\"Example sample: \", json.dumps(data_prev.__dict__['samples'][0].__dict__['metadata'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval\n",
    "I evaluated OpenAI’s \"gpt-4o-mini-2024-07-18\" model with temperature = 0, using the Inspect framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom scorer, to handle letter answers (2-hop) or multi-word answers (1-hop)\n",
    "@scorer(metrics=[accuracy(), stderr()])\n",
    "def acc_scorer(type='letter'):\n",
    "    if type == 'letter': # answer scorer for letter answers\n",
    "        return answer('letter')\n",
    "\n",
    "    async def score(state, target):\n",
    "        # answer scorer for potentially multi-word answers\n",
    "        output = state.output.completion.lower()\n",
    "        if 'answer:' in output:\n",
    "            output = output.split('answer:')[-1]\n",
    "        else:\n",
    "            output = output.split('\\n')[-1]\n",
    "        match = int(target.text.lower() in output)\n",
    "        return Score(value=match, answer=output)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define inspect task, both with and without chain of thought (cot)\n",
    "\n",
    "@task\n",
    "def multihop_task(n_hops=2, context=True, cot=False):\n",
    "    if context:\n",
    "        template = \"template.txt\"\n",
    "        no_cot_prompt = \"Answer the QUESTION, using the CONTEXT for assistance. Provide your answer at the end on its own line in the form \\\"ANSWER: $ANSWER\\\" (without quotes) where $ANSWER is the answer to the question.\"\n",
    "        cot_prompt = \"Answer the QUESTION, using the CONTEXT for assistance. Start your response with \\\"Let's think step by step\\\" and then provide your answer at the end on its own line in the form \\\"ANSWER: $ANSWER\\\" (without quotes) where $ANSWER is the answer to the question.\\n\"\n",
    "    else:\n",
    "        template = \"template_no_context.txt\"\n",
    "        no_cot_prompt = \"Answer the QUESTION. Provide your answer at the end on its own line in the form \\\"ANSWER: $ANSWER\\\" (without quotes) where $ANSWER is the answer to the question.\"\n",
    "        cot_prompt = \"Answer the QUESTION. Start your response with \\\"Let's think step by step\\\" and then provide your answer at the end on its own line in the form \\\"ANSWER: $ANSWER\\\" (without quotes) where $ANSWER is the answer to the question.\\n\"\n",
    "\n",
    "    if n_hops == 2:\n",
    "        dataset = data\n",
    "        type = 'letter'\n",
    "    else:\n",
    "        dataset = data_prev\n",
    "        type = 'multi-word'\n",
    "\n",
    "    if cot:\n",
    "        prompt = cot_prompt\n",
    "    else:\n",
    "        prompt = no_cot_prompt\n",
    "\n",
    "    return Task(\n",
    "        dataset = dataset,\n",
    "        solver = [prompt_template(template, instruction=prompt),\n",
    "                  generate()\n",
    "                  ],\n",
    "        scorer = [includes(), # checks if answer is anywhere in the output\n",
    "                  acc_scorer(type) # checks if answer appears after the 'ANSWER:' flag\n",
    "                  ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0d049c2ab44e9c8a142a3eb12d9b57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<inspect_ai._eval.task.task.Task object at 0x000001F552A3E110>, <inspect_ai._eval.task.task.Task object at 0x000001F553D8E8D0>, <inspect_ai._eval.task.task.Task object at 0x000001F555AA46D0>, <inspect_ai._eval.task.task.Task object at 0x000001F555AA40D0>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3401a3122f3454c96c13d4ff568dfc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c864b5d2c648a3b57f4e81bd50a274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8abb2ccac7a943bc96b9d2bab41ddd3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run evaluation\n",
    "tasks = [multihop_task(n_hops, context = context, cot = cot) for n_hops in (1,2) for context in (False,) for cot in (False, True)]\n",
    "print(tasks)\n",
    "evaluate_model(tasks, model = \"openai/gpt-4o-mini-2024-07-18\", temperature = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>epoch</th>\n",
       "      <th>input</th>\n",
       "      <th>choices</th>\n",
       "      <th>target</th>\n",
       "      <th>sandbox</th>\n",
       "      <th>files</th>\n",
       "      <th>setup</th>\n",
       "      <th>messages</th>\n",
       "      <th>output</th>\n",
       "      <th>...</th>\n",
       "      <th>metadata</th>\n",
       "      <th>store</th>\n",
       "      <th>events</th>\n",
       "      <th>model_usage</th>\n",
       "      <th>error</th>\n",
       "      <th>attachments</th>\n",
       "      <th>limit</th>\n",
       "      <th>cot</th>\n",
       "      <th>n_hops</th>\n",
       "      <th>has_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10cd47420baf11ebab90acde48001122_0</td>\n",
       "      <td>1</td>\n",
       "      <td>What is the first letter of the first name of ...</td>\n",
       "      <td>None</td>\n",
       "      <td>g</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[content='QUESTION: What is the first letter o...</td>\n",
       "      <td>model='gpt-4o-mini-2024-07-18' choices=[ChatCo...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'previous_question': 'Who is Joseph-André Tas...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[timestamp=datetime.datetime(2025, 1, 25, 23, ...</td>\n",
       "      <td>{'openai/gpt-4o-mini-2024-07-18': input_tokens...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'4ed799301fa5b64d307a64bf9b418ff3': 'What is ...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id  epoch   \n",
       "0  10cd47420baf11ebab90acde48001122_0      1  \\\n",
       "\n",
       "                                               input choices target sandbox   \n",
       "0  What is the first letter of the first name of ...    None      g    None  \\\n",
       "\n",
       "  files setup                                           messages   \n",
       "0  None  None  [content='QUESTION: What is the first letter o...  \\\n",
       "\n",
       "                                              output  ...   \n",
       "0  model='gpt-4o-mini-2024-07-18' choices=[ChatCo...  ...  \\\n",
       "\n",
       "                                            metadata store   \n",
       "0  {'previous_question': 'Who is Joseph-André Tas...    {}  \\\n",
       "\n",
       "                                              events   \n",
       "0  [timestamp=datetime.datetime(2025, 1, 25, 23, ...  \\\n",
       "\n",
       "                                         model_usage error   \n",
       "0  {'openai/gpt-4o-mini-2024-07-18': input_tokens...  None  \\\n",
       "\n",
       "                                         attachments limit   cot  n_hops   \n",
       "0  {'4ed799301fa5b64d307a64bf9b418ff3': 'What is ...  None  True       2  \\\n",
       "\n",
       "   has_context  \n",
       "0        False  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs = list_eval_logs()\n",
    "results = pd.DataFrame()\n",
    "for log in logs:\n",
    "    samps = read_eval_log(log).samples\n",
    "    df = pd.DataFrame([s.__dict__ for s in samps])\n",
    "    log_args = read_eval_log(log).eval.task_args\n",
    "    df['cot'] = log_args['cot']\n",
    "    df['n_hops'] = log_args['n_hops']\n",
    "    if 'context' in log_args:\n",
    "        df['has_context'] = log_args['context']\n",
    "    else:\n",
    "        df['has_context'] = True\n",
    "    results = pd.concat([results,df])\n",
    "results.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_includes_score (record):\n",
    "    return record['scores']['includes'].value == 'C'\n",
    "def get_answer_score (record):\n",
    "    return record['scores']['acc_scorer'].value in ['C',1]\n",
    "results['includes'] = results.apply(get_includes_score, axis=1)\n",
    "results['accuracy'] = results.apply(get_answer_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with context:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_hop</th>\n",
       "      <th>2_hop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>no_cot</th>\n",
       "      <td>95.348837</td>\n",
       "      <td>86.046512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cot</th>\n",
       "      <td>95.348837</td>\n",
       "      <td>93.023256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1_hop      2_hop\n",
       "no_cot  95.348837  86.046512\n",
       "cot     95.348837  93.023256"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without context:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_hop</th>\n",
       "      <th>2_hop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>no_cot</th>\n",
       "      <td>18.604651</td>\n",
       "      <td>27.906977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cot</th>\n",
       "      <td>20.930233</td>\n",
       "      <td>30.232558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1_hop      2_hop\n",
       "no_cot  18.604651  27.906977\n",
       "cot     20.930233  30.232558"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# accuracy scores\n",
    "for has_context in [True, False]:\n",
    "    acc = (results[results['has_context'] == has_context]\n",
    "           .pivot_table(index=['cot'], columns=['n_hops'],values=['accuracy'], aggfunc='mean')\n",
    "           *100)\n",
    "    acc.index = ['no_cot', 'cot']\n",
    "    acc.columns = ['1_hop', '2_hop']\n",
    "    print(f'Accuracy with{\"out\" if not has_context else \"\"} context:')\n",
    "    display(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy scores for gpt-4o-mini on 1-hop questions with context were exactly the same with and without CoT, so I won't do any further stats on those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Analysis - Wald Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_hops</th>\n",
       "      <th>has_context</th>\n",
       "      <th>cot</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0.302326</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.930233</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_hops  has_context    cot  accuracy   n\n",
       "0       1        False  False  0.186047  43\n",
       "1       1        False   True  0.209302  43\n",
       "2       1         True  False  0.953488  43\n",
       "3       1         True   True  0.953488  43\n",
       "4       2        False  False  0.279070  43\n",
       "5       2        False   True  0.302326  43\n",
       "6       2         True  False  0.860465  43\n",
       "7       2         True   True  0.930233  43"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_agg = results.groupby(['n_hops', 'has_context', 'cot']).agg({'accuracy': 'mean', 'id': 'count'}).rename(columns={'id':'n'}).reset_index()\n",
    "res_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Wald Test p-value function wrapper is borrowed from my previous work (also written by me).\n",
    "def p_value(data, target, comparisons=None, hypothesis=None, categories=['n_hops', 'has_context', 'cot'], verbose=True):\n",
    "    \"\"\"\n",
    "    Calculate a p-value for comparing the effect of categorical values on a target result.\n",
    "    Fits a linear model to the one-hot encoded categorical features.\n",
    "    Returns p-value(s) from Wald test to see if learned coefficients are significantly different.\n",
    "\n",
    "    - data is the table of results. Must contain a column 'n' with the number of trials per observation.\n",
    "    - target is the column name corresponding to the target data to fit to (this column should contain numeric values)\n",
    "    - Comparisons: List of (category colummn name, value1, value2) representing a comparison.\n",
    "    - hypothesis: To specify a single, more complicated Wald test. If hypothesis is provided, comparisons is ignored.\n",
    "    - categories: list of categorical features to include in the regression\n",
    "    \"\"\"\n",
    "    # Remove unnecessary columns and do one-hot encode categorical features\n",
    "    numericals = ['n',target]\n",
    "    data_encoded = pd.get_dummies(data[categories+numericals], columns=categories, drop_first=False)\n",
    "\n",
    "    # Format features to fit the model\n",
    "    X = data_encoded.drop(numericals, axis=1).astype(int)\n",
    "    y = data_encoded[target].astype(float)\n",
    "    if max(y) > 1: # convert percents to decimals\n",
    "        y = y/100\n",
    "\n",
    "    X = sm.add_constant(X)\n",
    "    # Sample weights indicating the number of trials for each observation\n",
    "    sample_weight = data_encoded['n'].astype(int)# Fit the logistic regression model using aggregated data\n",
    "    model = sm.GLM(y, X, family=sm.families.Binomial(), freq_weights=sample_weight)\n",
    "    results = model.fit()# Print the summary of the model\n",
    "    if verbose:\n",
    "        print(results.summary())\n",
    "\n",
    "    # perform Wald test to see if the coefficients are significantly different\n",
    "    p_values = {}\n",
    "    if hypothesis is None: # no specific hypothesis provided - use comparisons\n",
    "        for category, value1, value2 in comparisons:\n",
    "            hypothesis_string = f'{category}_{value1} = {category}_{value2}'\n",
    "            wald_test_result = results.wald_test(hypothesis_string, scalar=False)\n",
    "            if verbose:\n",
    "                print(wald_test_result)\n",
    "            p_values[hypothesis_string] = wald_test_result.pvalue, results.params\n",
    "    else: # use specific hypothesis\n",
    "        wald_test_result = results.wald_test(hypothesis, scalar=False)\n",
    "        if verbose:\n",
    "            print(wald_test_result)\n",
    "        p_values[hypothesis] = wald_test_result.pvalue, results.params\n",
    "    return p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:               accuracy   No. Observations:                    8\n",
      "Model:                            GLM   Df Residuals:                      340\n",
      "Model Family:                Binomial   Df Model:                            3\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:                -99.106\n",
      "Date:                Sat, 25 Jan 2025   Deviance:                       4.6445\n",
      "Time:                        23:40:48   Pearson chi2:                     4.79\n",
      "No. Iterations:                     5   Pseudo R-squ. (CS):              1.000\n",
      "Covariance Type:            nonrobust                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "const                 0.2754      0.068      4.061      0.000       0.143       0.408\n",
      "n_hops_1              0.0690      0.155      0.445      0.656      -0.235       0.373\n",
      "n_hops_2              0.2065      0.156      1.327      0.185      -0.099       0.511\n",
      "has_context_False    -1.6853      0.158    -10.684      0.000      -1.994      -1.376\n",
      "has_context_True      1.9607      0.188     10.442      0.000       1.593       2.329\n",
      "cot_False             0.0231      0.155      0.149      0.881      -0.281       0.327\n",
      "cot_True              0.2523      0.156      1.617      0.106      -0.054       0.558\n",
      "=====================================================================================\n",
      "<Wald test (chi2): statistic=[[0.20589681]], p-value=0.6500026256339859, df_denom=1>\n",
      "<Wald test (chi2): statistic=[[114.91272796]], p-value=8.223822353542468e-27, df_denom=1>\n",
      "<Wald test (chi2): statistic=[[0.57033664]], p-value=0.45012517516882633, df_denom=1>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_hops_1 = n_hops_2': (array(0.65000263),\n",
       "  const                0.275419\n",
       "  n_hops_1             0.068961\n",
       "  n_hops_2             0.206458\n",
       "  has_context_False   -1.685267\n",
       "  has_context_True     1.960686\n",
       "  cot_False            0.023101\n",
       "  cot_True             0.252318\n",
       "  dtype: float64),\n",
       " 'has_context_True = has_context_False': (array(8.22382235e-27),\n",
       "  const                0.275419\n",
       "  n_hops_1             0.068961\n",
       "  n_hops_2             0.206458\n",
       "  has_context_False   -1.685267\n",
       "  has_context_True     1.960686\n",
       "  cot_False            0.023101\n",
       "  cot_True             0.252318\n",
       "  dtype: float64),\n",
       " 'cot_True = cot_False': (array(0.45012518),\n",
       "  const                0.275419\n",
       "  n_hops_1             0.068961\n",
       "  n_hops_2             0.206458\n",
       "  has_context_False   -1.685267\n",
       "  has_context_True     1.960686\n",
       "  cot_False            0.023101\n",
       "  cot_True             0.252318\n",
       "  dtype: float64)}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_value(res_agg, 'accuracy', comparisons=[('n_hops', 1, 2), ('has_context', True, False), ('cot', True, False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are only statistically significant for **having the context in the prompt**, not the number of hops or use of chain of thought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
