{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install inspect-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from inspect_ai import Task, task\n",
    "from inspect_ai import eval as evaluate_model\n",
    "from inspect_ai.dataset import json_dataset, Sample\n",
    "from inspect_ai.scorer import includes,pattern, answer, Score, accuracy, stderr, scorer, Scorer, Target\n",
    "from inspect_ai.solver import generate, chain_of_thought,prompt_template\n",
    "from inspect_ai.log import list_eval_logs, read_eval_log\n",
    "from inspect_ai.solver._task_state import TaskState\n",
    "\n",
    "from statsmodels.stats.proportion import proportion_confint, test_proportions_2indep\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "## Source: [MoreHopQA final](https://github.com/Alab-NII/morehopqa/blob/main/datasets/files/morehopqa_final.json), letter-only\n",
    "I loaded the data from the github repo, filtered to questions which had a “letter” answer type, in order to have consistent answer parsing. This resulted in 43 multi-hop questions.\n",
    "I also parsed the provided “context” for each question into the following format:\n",
    "- {topic title}\n",
    "\t- {topic sentence}\n",
    "\t- {topic sentence}\n",
    "\n",
    "Note: I was originally hoping to use the MoreHopQA dataset without the context, but the model seemed unable to get answer any of the questions correctly due to factual recall errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 43 samples.\n",
      "Example sample:  {\n",
      "  \"previous_question\": \"Who was the producer of the 1994 American animated epic musical film which Mel Shaw animated?\",\n",
      "  \"previous_answer\": \"Don Hahn\",\n",
      "  \"no_of_hops\": 2,\n",
      "  \"answer_type\": \"letter\",\n",
      "  \"context\": \"Mel Shaw:\\n\\t- Mel Shaw (December 19, 1914 \\u2013 November 22, 2012) was an American animator, design artist, writer, and artist.\\n\\t- Shaw was involved in the animation, story design, and visual development of numerous Disney animated films, beginning with \\\"Bambi\\\", which was released in 1942.\\n\\t- His other animated film credits, usually involving animation design or the story, included \\\"The Rescuers\\\" in 1977, \\\"The Fox and the Hound\\\" in 1981, \\\"The Black Cauldron\\\" in 1985, \\\"The Great Mouse Detective\\\" in 1986, \\\"Beauty and the Beast\\\" in 1991, and \\\"The Lion King\\\" in 1994.\\n\\t- He was named a Disney Legend in 2004 for his contributions to the Walt Disney Company.\\n- The Lion King:\\n\\t- The Lion King is a 1994 American animated epic musical film produced by Walt Disney Feature Animation and released by Walt Disney Pictures.\\n\\t- It is the 32nd Disney animated feature film, and the fifth animated film produced during a period known as the Disney Renaissance.\\n\\t- \\\"The Lion King\\\" was directed by Roger Allers and Rob Minkoff, produced by Don Hahn, and has a screenplay credited to Irene Mecchi, Jonathan Roberts, and Linda Woolverton.\\n\\t- Its original songs were written by composer Elton John and lyricist Tim Rice, and original scores were written by Hans Zimmer.\\n\\t- The film features an ensemble voice cast that includes Matthew Broderick, James Earl Jones, Jeremy Irons, Jonathan Taylor Thomas, Moira Kelly, Nathan Lane, Ernie Sabella, Rowan Atkinson, Robert Guillaume, Madge Sinclair, Whoopi Goldberg, Cheech Marin, and Jim Cummings.\\n\\t- The story takes place in a kingdom of lions in Africa and was influenced by William Shakespeare's \\\"Hamlet\\\".\",\n",
      "  \"question\": \"What is the last letter of the last name of the producer of the 1994 American animated epic musical film which Mel Shaw animated?\",\n",
      "  \"answer\": \"n\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "demo_path = \"demo_data_letter.json\"\n",
    "og_data_path = os.path.abspath(os.path.join(\"..\", \"morehopqa\",\"datasets\",\"files\",\"morehopqa_final.json\"))\n",
    "if not os.path.exists(demo_path): # select 5 samples for demo\n",
    "    with open(og_data_path,'r', encoding='utf-8') as f:\n",
    "        d = json.load(f)\n",
    "    d_filt = [s for s in d if s['answer_type'] == 'letter'] # letter, to give it a better chance?\n",
    "    demo_data = d_filt\n",
    "    # demo_data = np.random.choice(d_filt,5,replace=False).tolist()\n",
    "    print(demo_data)\n",
    "    with open(demo_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(demo_data,f, indent=2)\n",
    "\n",
    "# load into inspect framework\n",
    "def process_morehopqa (record):\n",
    "    q = record['question']\n",
    "    context = '\\n- '.join(['\\n\\t- '.join([title+':']+[chunk.strip() for chunk in chunks])\n",
    "                           for title, chunks in record['context']])\n",
    "    a = record['answer']\n",
    "    meta = {k:record[k] for k in ['previous_question',\n",
    "                                                   'previous_answer',\n",
    "                                                   \"no_of_hops\",\n",
    "                                                   \"answer_type\"]}\n",
    "    meta['context'] = context\n",
    "    meta['question'] = q # for prompt template\n",
    "    meta['answer'] = a\n",
    "    return Sample(id=record[\"_id\"],\n",
    "                  input=q,\n",
    "                  target=a,\n",
    "                  metadata=meta\n",
    "                  )\n",
    "data = json_dataset(demo_path,\n",
    "                    sample_fields = process_morehopqa)\n",
    "\n",
    "# # display data\n",
    "print(f\"Dataset has {len(data)} samples.\")\n",
    "print(\"Example sample: \", json.dumps(data.__dict__['samples'][0].__dict__['metadata'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 43 samples.\n",
      "Example sample:  {\n",
      "  \"previous_question\": \"Who was the producer of the 1994 American animated epic musical film which Mel Shaw animated?\",\n",
      "  \"previous_answer\": \"Don Hahn\",\n",
      "  \"no_of_hops\": 1,\n",
      "  \"answer_type\": \"letter\",\n",
      "  \"context\": \"Mel Shaw:\\n\\t- Mel Shaw (December 19, 1914 \\u2013 November 22, 2012) was an American animator, design artist, writer, and artist.\\n\\t- Shaw was involved in the animation, story design, and visual development of numerous Disney animated films, beginning with \\\"Bambi\\\", which was released in 1942.\\n\\t- His other animated film credits, usually involving animation design or the story, included \\\"The Rescuers\\\" in 1977, \\\"The Fox and the Hound\\\" in 1981, \\\"The Black Cauldron\\\" in 1985, \\\"The Great Mouse Detective\\\" in 1986, \\\"Beauty and the Beast\\\" in 1991, and \\\"The Lion King\\\" in 1994.\\n\\t- He was named a Disney Legend in 2004 for his contributions to the Walt Disney Company.\\n- The Lion King:\\n\\t- The Lion King is a 1994 American animated epic musical film produced by Walt Disney Feature Animation and released by Walt Disney Pictures.\\n\\t- It is the 32nd Disney animated feature film, and the fifth animated film produced during a period known as the Disney Renaissance.\\n\\t- \\\"The Lion King\\\" was directed by Roger Allers and Rob Minkoff, produced by Don Hahn, and has a screenplay credited to Irene Mecchi, Jonathan Roberts, and Linda Woolverton.\\n\\t- Its original songs were written by composer Elton John and lyricist Tim Rice, and original scores were written by Hans Zimmer.\\n\\t- The film features an ensemble voice cast that includes Matthew Broderick, James Earl Jones, Jeremy Irons, Jonathan Taylor Thomas, Moira Kelly, Nathan Lane, Ernie Sabella, Rowan Atkinson, Robert Guillaume, Madge Sinclair, Whoopi Goldberg, Cheech Marin, and Jim Cummings.\\n\\t- The story takes place in a kingdom of lions in Africa and was influenced by William Shakespeare's \\\"Hamlet\\\".\",\n",
      "  \"question\": \"Who was the producer of the 1994 American animated epic musical film which Mel Shaw animated?\",\n",
      "  \"answer\": \"Don Hahn\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# get previous question (1 hop)\n",
    "def get_prev(record):\n",
    "    q = record['previous_question']\n",
    "    a = record['previous_answer']\n",
    "    meta = {k:record[k] for k in ['previous_question',\n",
    "                                                   'previous_answer',\n",
    "                                                   \"no_of_hops\",\n",
    "                                                   \"answer_type\"]}\n",
    "    meta['no_of_hops'] = meta['no_of_hops'] - 1\n",
    "    context = '\\n- '.join(['\\n\\t- '.join([title+':']+[chunk.strip() for chunk in chunks])\n",
    "                           for title, chunks in record['context']])\n",
    "    meta['context'] = context\n",
    "    meta['question'] = q # for prompt template\n",
    "    meta['answer'] = a\n",
    "    return Sample(id=record[\"_id\"],\n",
    "                  input=q,\n",
    "                  target=a,\n",
    "                  metadata=meta\n",
    "                  )\n",
    "data_prev = json_dataset(demo_path,\n",
    "                         get_prev)\n",
    "print(f\"Dataset has {len(data_prev)} samples.\")\n",
    "print(\"Example sample: \", json.dumps(data_prev.__dict__['samples'][0].__dict__['metadata'], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval\n",
    "I evaluated OpenAI’s \"gpt-4o-mini-2024-07-18\" model with temperature = 0, using the Inspect framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom scorer, to handle letter answers (2-hop) or multi-word answers (1-hop)\n",
    "@scorer(metrics=[accuracy(), stderr()])\n",
    "def acc_scorer(type='letter'):\n",
    "    if type == 'letter': # answer scorer for letter answers\n",
    "        return answer('letter')\n",
    "\n",
    "    async def score(state, target):\n",
    "        # answer scorer for potentially multi-word answers\n",
    "        output = state.output.completion.lower()\n",
    "        if 'answer:' in output:\n",
    "            output = output.split('answer:')[-1]\n",
    "        else:\n",
    "            output = output.split('\\n')[-1]\n",
    "        match = int(target.text.lower() in output)\n",
    "        return Score(value=match, answer=output)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define inspect task, both with and without chain of thought (cot)\n",
    "template = \"template.txt\"\n",
    "no_cot_prompt = \"Answer the QUESTION, using the CONTEXT for assistance. Provide your answer at the end on its own line in the form \\\"ANSWER: $ANSWER\\\" (without quotes) where $ANSWER is the answer to the question.\"\n",
    "cot_prompt = \"Answer the QUESTION, using the CONTEXT for assistance. Start your response with \\\"Let's think step by step\\\" and then provide your answer at the end on its own line in the form \\\"ANSWER: $ANSWER\\\" (without quotes) where $ANSWER is the answer to the question.\\n\"\n",
    "\n",
    "@task\n",
    "def multihop_task(n_hops=2, cot=False):\n",
    "    if n_hops == 2:\n",
    "        dataset = data\n",
    "        type = 'letter'\n",
    "    else:\n",
    "        dataset = data_prev\n",
    "        type = 'multi-word'\n",
    "    if cot:\n",
    "        prompt = cot_prompt\n",
    "    else:\n",
    "        prompt = no_cot_prompt\n",
    "    return Task(\n",
    "        dataset = dataset,\n",
    "        solver = [prompt_template(template, instruction=prompt),\n",
    "                  generate()\n",
    "                  ],\n",
    "        scorer = [includes(), # checks if answer is anywhere in the output\n",
    "                  acc_scorer(type) # checks if answer appears after the 'ANSWER:' flag\n",
    "                  ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<inspect_ai._eval.task.task.Task object at 0x000001F553EB7750>, <inspect_ai._eval.task.task.Task object at 0x000001F55584B110>, <inspect_ai._eval.task.task.Task object at 0x000001F555849450>, <inspect_ai._eval.task.task.Task object at 0x000001F555848DD0>]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\eviye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inspect_ai\\log\\_transcript.py:378: \n",
       "RuntimeWarning: coroutine 'task_run_sample' was never awaited\n",
       "  with track_state_changes(type), track_store_changes():\n",
       "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\eviye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inspect_ai\\log\\_transcript.py:378: \n",
       "RuntimeWarning: coroutine 'task_run_sample' was never awaited\n",
       "  with track_state_changes(type), track_store_changes():\n",
       "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">c:\\Users\\eviye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inspect_ai\\log\\_transcript.py:378: \n",
       "RuntimeWarning: coroutine 'pattern.&lt;locals&gt;.score' was never awaited\n",
       "  with track_state_changes(type), track_store_changes():\n",
       "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
       "</pre>\n"
      ],
      "text/plain": [
       "c:\\Users\\eviye\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\inspect_ai\\log\\_transcript.py:378: \n",
       "RuntimeWarning: coroutine 'pattern.<locals>.score' was never awaited\n",
       "  with track_state_changes(type), track_store_changes():\n",
       "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35cbb008d6c4958ae6b4bd6d022b982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a94d531c544c558238bad5489c7ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4314703a87964ebc8e8254fb752d9975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb204675b9c74140ac057185e5a98902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run evaluation\n",
    "tasks = [multihop_task(n_hops, cot = cot) for n_hops in (1,2) for cot in (False, True)]\n",
    "print(tasks)\n",
    "evaluate_model(tasks, model = \"openai/gpt-4o-mini-2024-07-18\", temperature = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>epoch</th>\n",
       "      <th>input</th>\n",
       "      <th>choices</th>\n",
       "      <th>target</th>\n",
       "      <th>sandbox</th>\n",
       "      <th>files</th>\n",
       "      <th>setup</th>\n",
       "      <th>messages</th>\n",
       "      <th>output</th>\n",
       "      <th>scores</th>\n",
       "      <th>metadata</th>\n",
       "      <th>store</th>\n",
       "      <th>events</th>\n",
       "      <th>model_usage</th>\n",
       "      <th>error</th>\n",
       "      <th>attachments</th>\n",
       "      <th>limit</th>\n",
       "      <th>cot</th>\n",
       "      <th>n_hops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10cd47420baf11ebab90acde48001122_0</td>\n",
       "      <td>1</td>\n",
       "      <td>What is the first letter of the first name of ...</td>\n",
       "      <td>None</td>\n",
       "      <td>g</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[content='CONTEXT:\\n- Thomas-Pierre-Joseph Tas...</td>\n",
       "      <td>model='gpt-4o-mini-2024-07-18' choices=[ChatCo...</td>\n",
       "      <td>{'includes': value=1 answer='Let\\'s think step...</td>\n",
       "      <td>{'previous_question': 'Who is Joseph-André Tas...</td>\n",
       "      <td>{}</td>\n",
       "      <td>[timestamp=datetime.datetime(2025, 1, 25, 22, ...</td>\n",
       "      <td>{'openai/gpt-4o-mini-2024-07-18': input_tokens...</td>\n",
       "      <td>None</td>\n",
       "      <td>{'4ed799301fa5b64d307a64bf9b418ff3': 'What is ...</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id  epoch   \n",
       "0  10cd47420baf11ebab90acde48001122_0      1  \\\n",
       "\n",
       "                                               input choices target sandbox   \n",
       "0  What is the first letter of the first name of ...    None      g    None  \\\n",
       "\n",
       "  files setup                                           messages   \n",
       "0  None  None  [content='CONTEXT:\\n- Thomas-Pierre-Joseph Tas...  \\\n",
       "\n",
       "                                              output   \n",
       "0  model='gpt-4o-mini-2024-07-18' choices=[ChatCo...  \\\n",
       "\n",
       "                                              scores   \n",
       "0  {'includes': value=1 answer='Let\\'s think step...  \\\n",
       "\n",
       "                                            metadata store   \n",
       "0  {'previous_question': 'Who is Joseph-André Tas...    {}  \\\n",
       "\n",
       "                                              events   \n",
       "0  [timestamp=datetime.datetime(2025, 1, 25, 22, ...  \\\n",
       "\n",
       "                                         model_usage error   \n",
       "0  {'openai/gpt-4o-mini-2024-07-18': input_tokens...  None  \\\n",
       "\n",
       "                                         attachments limit   cot  n_hops  \n",
       "0  {'4ed799301fa5b64d307a64bf9b418ff3': 'What is ...  None  True       2  "
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs = list_eval_logs()\n",
    "results = pd.DataFrame()\n",
    "for log in logs:\n",
    "    samps = read_eval_log(log).samples\n",
    "    df = pd.DataFrame([s.__dict__ for s in samps])\n",
    "    df['cot'] = read_eval_log(log).eval.task_args['cot']\n",
    "    df['n_hops'] = read_eval_log(log).eval.task_args['n_hops']\n",
    "    results = pd.concat([results,df])\n",
    "results.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_includes_score (record):\n",
    "    return record['scores']['includes'].value == 'C'\n",
    "def get_answer_score (record):\n",
    "    return record['scores']['acc_scorer'].value in ['C',1]\n",
    "results['includes'] = results.apply(get_includes_score, axis=1)\n",
    "results['accuracy'] = results.apply(get_answer_score, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_hop</th>\n",
       "      <th>2_hop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>no_cot</th>\n",
       "      <td>95.348837</td>\n",
       "      <td>86.046512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cot</th>\n",
       "      <td>95.348837</td>\n",
       "      <td>93.023256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            1_hop      2_hop\n",
       "no_cot  95.348837  86.046512\n",
       "cot     95.348837  93.023256"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy scores\n",
    "acc = results.pivot_table(index=['cot'], columns=['n_hops'],values=['accuracy'], aggfunc='mean')*100\n",
    "acc.index = ['no_cot', 'cot']\n",
    "acc.columns = ['1_hop', '2_hop']\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy scores for gpt-4o-mini on 1-hop questions were exactly the same with and without CoT, so I won't do any further stats on those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(cot\n",
       " False    0.722968\n",
       " True     0.815017\n",
       " dtype: float64,\n",
       " cot\n",
       " False    0.937444\n",
       " True     0.980715\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 95% binomial confidence intervals for accuracy (2-hop)\n",
    "(proportion_confint(results[results['n_hops'] == 2].groupby('cot')['accuracy'].agg('sum'),\n",
    "                    nobs=results[results['n_hops'] == 2].shape[0]/2,\n",
    "                    method='binom_test',\n",
    "                    alpha=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contingency table: [[3, 3], [0, 37]]\n",
      "McNemar test results:\n",
      "pvalue      0.25\n",
      "statistic   0.0\n"
     ]
    }
   ],
   "source": [
    "# mcnemar test on accuracy (2-hop)\n",
    "pivoted = results[results['n_hops'] == 2].pivot(index='id', columns='cot', values='accuracy')\n",
    "\n",
    "contingency = [[0,0],[0,0]]\n",
    "for i, row in pivoted.iterrows():\n",
    "    contingency[int(row[False])][int(row[True])] += 1 # False is no_cot, True is cot\n",
    "print('Contingency table:', contingency)\n",
    "print('McNemar test results:')\n",
    "print(mcnemar(contingency, exact=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are **not statistically significant**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
